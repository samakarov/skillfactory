{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:05.920640Z",
     "iopub.status.busy": "2021-11-15T21:22:05.918637Z",
     "iopub.status.idle": "2021-11-15T21:22:16.419512Z",
     "shell.execute_reply": "2021-11-15T21:22:16.420087Z",
     "shell.execute_reply.started": "2021-11-15T21:08:22.297707Z"
    },
    "papermill": {
     "duration": 10.525562,
     "end_time": "2021-11-15T21:22:16.420421",
     "exception": false,
     "start_time": "2021-11-15T21:22:05.894859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Main settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:16.641247Z",
     "iopub.status.busy": "2021-11-15T21:22:16.640308Z",
     "iopub.status.idle": "2021-11-15T21:22:16.645035Z",
     "shell.execute_reply": "2021-11-15T21:22:16.644405Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.004539Z"
    },
    "papermill": {
     "duration": 0.207878,
     "end_time": "2021-11-15T21:22:16.645193",
     "exception": false,
     "start_time": "2021-11-15T21:22:16.437315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 96\n",
    "PAD_ID = 1\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    PATH+'vocab-roberta-base.json',\n",
    "    PATH+'merges-roberta-base.txt',\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:16.704603Z",
     "iopub.status.busy": "2021-11-15T21:22:16.685587Z",
     "iopub.status.idle": "2021-11-15T21:22:16.707973Z",
     "shell.execute_reply": "2021-11-15T21:22:16.707039Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.147123Z"
    },
    "papermill": {
     "duration": 0.047028,
     "end_time": "2021-11-15T21:22:16.708108",
     "exception": false,
     "start_time": "2021-11-15T21:22:16.661080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_train():\n",
    "    \"\"\"\n",
    "        Load train dataset\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\n",
    "        '../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "    train['text'] = train['text'].astype(str)\n",
    "    train['selected_text'] = train['selected_text'].astype(str)\n",
    "    return train\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    \"\"\"\n",
    "        Load test dataset\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(\n",
    "        '../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "    test['text'] = test['text'].astype(str)\n",
    "    return test\n",
    "\n",
    "\n",
    "def read_submission():\n",
    "    \"\"\"\n",
    "        Load submission template\n",
    "    \"\"\"\n",
    "    sub = pd.read_csv(\n",
    "        '../input/tweet-sentiment-extraction/sample_submission.csv').fillna('')\n",
    "    return sub\n",
    "\n",
    "\n",
    "def jaccard_improve(str1, str2):\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    index = str1.find(str2)\n",
    "    text1 = str1[:index]\n",
    "\n",
    "    text2 = str1[index:].replace(str2, '')\n",
    "    words1 = text1.split()\n",
    "    words2 = text2.split()\n",
    "\n",
    "    if len(words1) > len(words2):\n",
    "        words1 = words1[-3:]\n",
    "        mod_text = \" \".join(words1)+\" \" + str2\n",
    "    else:\n",
    "        words2 = words2[0:2]\n",
    "        mod_text = str2+\" \"+\" \".join(words2)\n",
    "    return mod_text\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "       Tune loss function for Model \n",
    "    \"\"\"\n",
    "    # adjust the targets for sequence bucketing\n",
    "    l = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :l]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "                                                    from_logits=False, \n",
    "                                                    label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    \"\"\"Tune sceduler for learning rate\"\"\"\n",
    "    return 3e-5 * 0.2**epoch\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "        Builds Model and tune shape for inputs.\n",
    "        As main part of Model we use pretrained roDerta model.\n",
    "        And two heads with Convolution layers.\n",
    "    \"\"\"\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(\n",
    "        PATH+'pretrained-roberta-base.h5', config=config)\n",
    "\n",
    "    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n",
    "\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x2 = tf.keras.layers.Conv1D(768, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    \"\"\"\n",
    "        Calculates Jaccard score\n",
    "    \"\"\"\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a) == 0) & (len(b) == 0):\n",
    "        return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:16.744786Z",
     "iopub.status.busy": "2021-11-15T21:22:16.743844Z",
     "iopub.status.idle": "2021-11-15T21:22:16.964717Z",
     "shell.execute_reply": "2021-11-15T21:22:16.964051Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.175191Z"
    },
    "papermill": {
     "duration": 0.240888,
     "end_time": "2021-11-15T21:22:16.964896",
     "exception": false,
     "start_time": "2021-11-15T21:22:16.724008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = read_train()\n",
    "test = read_test()\n",
    "submission_df = read_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prepare data for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in previous notebook (BiLSTM), \"neutral\" part has least influence at result, so we will train our model only with \"positive\" and \"negative\" tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:17.016689Z",
     "iopub.status.busy": "2021-11-15T21:22:17.015427Z",
     "iopub.status.idle": "2021-11-15T21:22:17.028661Z",
     "shell.execute_reply": "2021-11-15T21:22:17.028009Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.354171Z"
    },
    "papermill": {
     "duration": 0.038761,
     "end_time": "2021-11-15T21:22:17.028842",
     "exception": false,
     "start_time": "2021-11-15T21:22:16.990081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non neutral train data: (16363, 4)\n",
      "non neutral test data: (2104, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = train.loc[train.sentiment !=\n",
    "                     \"neutral\"].reset_index(drop=True, inplace=False)\n",
    "test_df = test.loc[test.sentiment != \"neutral\"].reset_index(\n",
    "    drop=True, inplace=False)\n",
    "print(f\"non neutral train data: {train_df.shape}\")\n",
    "print(f\"non neutral test data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make templates for Model's inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:17.069800Z",
     "iopub.status.busy": "2021-11-15T21:22:17.068694Z",
     "iopub.status.idle": "2021-11-15T21:22:17.076074Z",
     "shell.execute_reply": "2021-11-15T21:22:17.075451Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.375074Z"
    },
    "papermill": {
     "duration": 0.031258,
     "end_time": "2021-11-15T21:22:17.076255",
     "exception": false,
     "start_time": "2021-11-15T21:22:17.044997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16363, 96)\n",
      "(16363, 96)\n",
      "(16363, 96)\n",
      "(16363, 96)\n",
      "(16363, 96)\n"
     ]
    }
   ],
   "source": [
    "ct = train_df.shape[0]\n",
    "input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "start_tokens = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "end_tokens = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(token_type_ids.shape)\n",
    "print(start_tokens.shape)\n",
    "print(end_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:17.118745Z",
     "iopub.status.busy": "2021-11-15T21:22:17.117491Z",
     "iopub.status.idle": "2021-11-15T21:22:24.843031Z",
     "shell.execute_reply": "2021-11-15T21:22:24.842102Z",
     "shell.execute_reply.started": "2021-11-15T21:08:31.388964Z"
    },
    "papermill": {
     "duration": 7.75101,
     "end_time": "2021-11-15T21:22:24.843179",
     "exception": false,
     "start_time": "2021-11-15T21:22:17.092169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(train_df.shape[0]):\n",
    "\n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train_df.loc[k, 'text'].split())\n",
    "    text2 = \" \".join(train_df.loc[k, 'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)] = 1\n",
    "    text1[idx-1] == ' '\n",
    "    if text1[idx-1] == ' ':\n",
    "        chars[idx-1] = 1\n",
    "    enc = tokenizer.encode(text1)\n",
    "\n",
    "    # ID_OFFSETS\n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx, idx+len(w)))\n",
    "        idx += len(w)\n",
    "\n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i, (a, b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm > 0:\n",
    "            toks.append(i)\n",
    "\n",
    "    # Insert Paddings and separators\n",
    "    s_tok = sentiment_id[train_df.loc[k, 'sentiment']]\n",
    "    input_ids[k, :len(enc.ids)+5] = [0] + [s_tok] + [2, 2] + enc.ids + [2]\n",
    "    attention_mask[k, :len(enc.ids)+5] = 1\n",
    "    if len(toks) > 0:\n",
    "        start_tokens[k, toks[0]+1] = 1\n",
    "        end_tokens[k, toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:24.885333Z",
     "iopub.status.busy": "2021-11-15T21:22:24.884001Z",
     "iopub.status.idle": "2021-11-15T21:22:25.193170Z",
     "shell.execute_reply": "2021-11-15T21:22:25.193682Z",
     "shell.execute_reply.started": "2021-11-15T21:08:36.440700Z"
    },
    "papermill": {
     "duration": 0.333856,
     "end_time": "2021-11-15T21:22:25.193906",
     "exception": false,
     "start_time": "2021-11-15T21:22:24.860050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ct = test_df.shape[0]\n",
    "input_ids_t = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "attention_mask_t = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "\n",
    "for k in range(test_df.shape[0]):\n",
    "\n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test_df.loc[k, 'text'].split())\n",
    "\n",
    "    enc = tokenizer.encode(text1)\n",
    "    s_tok = sentiment_id[test_df.loc[k, 'sentiment']]\n",
    "    input_ids_t[k, :len(enc.ids)+5] = [0] + [s_tok] + [2, 2] + enc.ids + [2]\n",
    "    attention_mask_t[k, :len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T21:22:25.259704Z",
     "iopub.status.busy": "2021-11-15T21:22:25.231549Z",
     "iopub.status.idle": "2021-11-15T22:13:31.304905Z",
     "shell.execute_reply": "2021-11-15T22:13:31.305659Z",
     "shell.execute_reply.started": "2021-11-15T21:10:52.399478Z"
    },
    "papermill": {
     "duration": 3066.095009,
     "end_time": "2021-11-15T22:13:31.305906",
     "exception": false,
     "start_time": "2021-11-15T21:22:25.210897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 21:22:25.371830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:25.373472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:25.374552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:25.375927: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-15 21:22:25.377125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:25.378199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:25.379262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:30.653673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:30.654884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:30.656039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 21:22:30.656974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "2021-11-15 21:22:48.544077: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 21:23:09.120647: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/410 [==============================] - 207s 444ms/step - loss: 4.3120 - activation_loss: 2.1739 - activation_1_loss: 2.1381 - val_loss: 3.7282 - val_activation_loss: 1.9257 - val_activation_1_loss: 1.8025\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.72824, saving model to v3-roberta-0.h5\n",
      "Epoch 2/3\n",
      "410/410 [==============================] - 178s 435ms/step - loss: 3.6200 - activation_loss: 1.8691 - activation_1_loss: 1.7509 - val_loss: 3.6519 - val_activation_loss: 1.8785 - val_activation_1_loss: 1.7734\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.72824 to 3.65186, saving model to v3-roberta-0.h5\n",
      "Epoch 3/3\n",
      "410/410 [==============================] - 179s 436ms/step - loss: 3.5176 - activation_loss: 1.8198 - activation_1_loss: 1.6978 - val_loss: 3.6515 - val_activation_loss: 1.8806 - val_activation_1_loss: 1.7708\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.65186 to 3.65146, saving model to v3-roberta-0.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "103/103 [==============================] - 19s 148ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.5337233023417904\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "410/410 [==============================] - 202s 446ms/step - loss: 4.3630 - activation_loss: 2.1933 - activation_1_loss: 2.1697 - val_loss: 3.8258 - val_activation_loss: 1.9761 - val_activation_1_loss: 1.8497\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.82585, saving model to v3-roberta-1.h5\n",
      "Epoch 2/3\n",
      "410/410 [==============================] - 179s 437ms/step - loss: 3.6653 - activation_loss: 1.8897 - activation_1_loss: 1.7756 - val_loss: 3.6779 - val_activation_loss: 1.8771 - val_activation_1_loss: 1.8009\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.82585 to 3.67795, saving model to v3-roberta-1.h5\n",
      "Epoch 3/3\n",
      "410/410 [==============================] - 179s 437ms/step - loss: 3.5607 - activation_loss: 1.8384 - activation_1_loss: 1.7223 - val_loss: 3.6816 - val_activation_loss: 1.8762 - val_activation_1_loss: 1.8054\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.67795\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "103/103 [==============================] - 20s 149ms/step\n",
      ">>>> FOLD 2 Jaccard = 0.5276962230252922\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "410/410 [==============================] - 202s 446ms/step - loss: 4.4070 - activation_loss: 2.2053 - activation_1_loss: 2.2017 - val_loss: 3.7561 - val_activation_loss: 1.9287 - val_activation_1_loss: 1.8273\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.75606, saving model to v3-roberta-2.h5\n",
      "Epoch 2/3\n",
      "410/410 [==============================] - 179s 437ms/step - loss: 3.6580 - activation_loss: 1.8820 - activation_1_loss: 1.7760 - val_loss: 3.6963 - val_activation_loss: 1.9011 - val_activation_1_loss: 1.7951\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.75606 to 3.69625, saving model to v3-roberta-2.h5\n",
      "Epoch 3/3\n",
      "410/410 [==============================] - 180s 438ms/step - loss: 3.5682 - activation_loss: 1.8391 - activation_1_loss: 1.7292 - val_loss: 3.6967 - val_activation_loss: 1.9072 - val_activation_1_loss: 1.7896\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.69625\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "103/103 [==============================] - 19s 151ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.5208550696760247\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "410/410 [==============================] - 204s 450ms/step - loss: 4.3192 - activation_loss: 2.1592 - activation_1_loss: 2.1599 - val_loss: 3.7662 - val_activation_loss: 1.9375 - val_activation_1_loss: 1.8287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.76618, saving model to v3-roberta-3.h5\n",
      "Epoch 2/3\n",
      "410/410 [==============================] - 180s 439ms/step - loss: 3.6362 - activation_loss: 1.8691 - activation_1_loss: 1.7670 - val_loss: 3.7347 - val_activation_loss: 1.9272 - val_activation_1_loss: 1.8075\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.76618 to 3.73472, saving model to v3-roberta-3.h5\n",
      "Epoch 3/3\n",
      "410/410 [==============================] - 180s 439ms/step - loss: 3.5472 - activation_loss: 1.8266 - activation_1_loss: 1.7205 - val_loss: 3.7217 - val_activation_loss: 1.9179 - val_activation_1_loss: 1.8038\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.73472 to 3.72169, saving model to v3-roberta-3.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "103/103 [==============================] - 19s 149ms/step\n",
      ">>>> FOLD 4 Jaccard = 0.5226844958409589\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "410/410 [==============================] - 203s 447ms/step - loss: 4.3586 - activation_loss: 2.1815 - activation_1_loss: 2.1770 - val_loss: 3.8560 - val_activation_loss: 1.9723 - val_activation_1_loss: 1.8836\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.85596, saving model to v3-roberta-4.h5\n",
      "Epoch 2/3\n",
      "410/410 [==============================] - 180s 440ms/step - loss: 3.6436 - activation_loss: 1.8794 - activation_1_loss: 1.7641 - val_loss: 3.7097 - val_activation_loss: 1.9037 - val_activation_1_loss: 1.8061\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.85596 to 3.70972, saving model to v3-roberta-4.h5\n",
      "Epoch 3/3\n",
      "410/410 [==============================] - 180s 438ms/step - loss: 3.5252 - activation_loss: 1.8185 - activation_1_loss: 1.7067 - val_loss: 3.7109 - val_activation_loss: 1.9069 - val_activation_1_loss: 1.8040\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.70972\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "103/103 [==============================] - 22s 147ms/step\n",
      ">>>> FOLD 5 Jaccard = 0.5233789775598878\n",
      "\n",
      ">>>> OVERALL 5Fold CV Jaccard = 0.5256676136887908\n"
     ]
    }
   ],
   "source": [
    "jac = []\n",
    "VER = 'v3'\n",
    "DISPLAY = 1  # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0], MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0], MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=777)\n",
    "for fold, (idxT, idxV) in enumerate(skf.split(input_ids, train_df.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i' % (fold+1))\n",
    "    print('#'*25)\n",
    "\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5' % (VER, fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "    hist = model.fit([input_ids[idxT, ], attention_mask[idxT, ], token_type_ids[idxT, ]],\n",
    "                     [start_tokens[idxT, ], end_tokens[idxT, ]],\n",
    "                     epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=DISPLAY,\n",
    "                     callbacks=[sv, reduce_lr],\n",
    "                     validation_data=([input_ids[idxV, ], attention_mask[idxV, ], token_type_ids[idxV, ]],\n",
    "                                      [start_tokens[idxV, ], end_tokens[idxV, ]]))\n",
    "\n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5' % (VER, fold))\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV, ], oof_end[idxV, ] = model.predict(\n",
    "        [input_ids[idxV, ], attention_mask[idxV, ], token_type_ids[idxV, ]], verbose=DISPLAY)\n",
    "\n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k, ])\n",
    "        b = np.argmax(oof_end[k, ])\n",
    "        if a > b:\n",
    "            # IMPROVE CV/LB with better choice here\n",
    "            st = train_df.loc[k, 'text']\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train_df.loc[k, 'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st, train_df.loc[k, 'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard =' % (fold+1), np.mean(all))\n",
    "    print()\n",
    "\n",
    "\n",
    "print('>>>> OVERALL 5Fold CV Jaccard =', np.mean(jac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:13:35.958823Z",
     "iopub.status.busy": "2021-11-15T22:13:35.957691Z",
     "iopub.status.idle": "2021-11-15T22:16:07.242416Z",
     "shell.execute_reply": "2021-11-15T22:16:07.241419Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.653390Z"
    },
    "papermill": {
     "duration": 153.633102,
     "end_time": "2021-11-15T22:16:07.242571",
     "exception": false,
     "start_time": "2021-11-15T22:13:33.609469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### MODEL 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "66/66 [==============================] - 13s 148ms/step\n",
      "#########################\n",
      "### MODEL 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "66/66 [==============================] - 13s 150ms/step\n",
      "#########################\n",
      "### MODEL 3\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "66/66 [==============================] - 14s 160ms/step\n",
      "#########################\n",
      "### MODEL 4\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "66/66 [==============================] - 14s 163ms/step\n",
      "#########################\n",
      "### MODEL 5\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test...\n",
      "66/66 [==============================] - 14s 159ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "DISPLAY = 1\n",
    "for i in range(5):\n",
    "    print('#'*25)\n",
    "    print('### MODEL %i' % (i+1))\n",
    "    print('#'*25)\n",
    "\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    # model.load_weights('../input/model-v3/v3-roberta-%i.h5'%i)\n",
    "    model.load_weights('./v3-roberta-%i.h5' % i)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t, attention_mask_t,\n",
    "                           token_type_ids_t], verbose=DISPLAY)\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get text by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:16:12.496337Z",
     "iopub.status.busy": "2021-11-15T22:16:12.495355Z",
     "iopub.status.idle": "2021-11-15T22:16:12.859498Z",
     "shell.execute_reply": "2021-11-15T22:16:12.858830Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.655280Z"
    },
    "papermill": {
     "duration": 3.112825,
     "end_time": "2021-11-15T22:16:12.859644",
     "exception": false,
     "start_time": "2021-11-15T22:16:09.746819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k, ])\n",
    "    b = np.argmax(preds_end[k, ])\n",
    "    if a > b:\n",
    "        st = test_df.loc[k, 'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test_df.loc[k, 'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:16:17.831097Z",
     "iopub.status.busy": "2021-11-15T22:16:17.829959Z",
     "iopub.status.idle": "2021-11-15T22:16:17.835623Z",
     "shell.execute_reply": "2021-11-15T22:16:17.835056Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.657303Z"
    },
    "papermill": {
     "duration": 2.506588,
     "end_time": "2021-11-15T22:16:17.835795",
     "exception": false,
     "start_time": "2021-11-15T22:16:15.329207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[\"selected_text\"] = \"\"  # make a column for predictions\n",
    "\n",
    "# make predictions for neutral sentiment\n",
    "test.loc[test.sentiment == \"neutral\",\n",
    "         \"selected_text\"] = test.loc[test.sentiment == \"neutral\", \"text\"]\n",
    "\n",
    "# make predictions for neutral sentiment\n",
    "test.loc[test.sentiment != \"neutral\", \"selected_text\"] = all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:16:23.261913Z",
     "iopub.status.busy": "2021-11-15T22:16:23.254937Z",
     "iopub.status.idle": "2021-11-15T22:16:23.286414Z",
     "shell.execute_reply": "2021-11-15T22:16:23.284528Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.659026Z"
    },
    "papermill": {
     "duration": 2.950926,
     "end_time": "2021-11-15T22:16:23.286621",
     "exception": false,
     "start_time": "2021-11-15T22:16:20.335695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3534, 4), (2104, 3), (3534, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test_df.shape, submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:16:28.832653Z",
     "iopub.status.busy": "2021-11-15T22:16:28.831410Z",
     "iopub.status.idle": "2021-11-15T22:16:28.850639Z",
     "shell.execute_reply": "2021-11-15T22:16:28.850098Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.661705Z"
    },
    "papermill": {
     "duration": 2.541696,
     "end_time": "2021-11-15T22:16:28.850830",
     "exception": false,
     "start_time": "2021-11-15T22:16:26.309134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_df['selected_text'] = all\n",
    "test[['textID', 'selected_text']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-15T22:16:33.824975Z",
     "iopub.status.busy": "2021-11-15T22:16:33.823825Z",
     "iopub.status.idle": "2021-11-15T22:16:33.837605Z",
     "shell.execute_reply": "2021-11-15T22:16:33.836928Z",
     "shell.execute_reply.started": "2021-11-15T21:08:55.663597Z"
    },
    "papermill": {
     "duration": 2.512465,
     "end_time": "2021-11-15T22:16:33.837787",
     "exception": false,
     "start_time": "2021-11-15T22:16:31.325322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment  \\\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3  01082688c6                                        happy bday!  positive   \n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "\n",
       "                                       selected_text  \n",
       "0  Last session of the day  http://twitpic.com/67ezh  \n",
       "1                                           exciting  \n",
       "2                                      such a shame!  \n",
       "3                                              happy  \n",
       "4                                        i like it!!  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.499068,
     "end_time": "2021-11-15T22:16:39.128912",
     "exception": false,
     "start_time": "2021-11-15T22:16:36.629844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "During work on this part we try to use different type of text preprocessing, described in EDA and BiLSTM notebooks. Finally, we desided, that any preprocessing not give positive result at all. Also, we use different type of heads. Simple Dense layes give good result and more complex head let's us improve result at 10-15%. Obtained result: 0.71141/0.70859\n",
    "\n",
    "What next:\n",
    "1. You can use ansible with different models\n",
    "2. You can use different text augumentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3288.239891,
   "end_time": "2021-11-15T22:16:45.010956",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-15T21:21:56.771065",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
